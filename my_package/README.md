## my_package/Limits_of_authority.py

🔎
```bash
my_package/Limits_of_authority.py
```

实现了简单的权限管理，能够根据用户的权限控制不同的操作，并包含了文件完整性验证的功能。

# 功能介绍：

1.权限管理:

- 定义了不同的权限级别，包括 普通用户、受限操作 和 高级操作。

- 定义了每个操作所需的权限级别，例如 查看数据 需要 普通用户 权限，修改配置 需要 受限操作 权限，核心操作 需要 高级操作 权限。

- 使用 SHA256 哈希算法对用户密钥进行加密存储，并通过比较输入密钥的哈希值进行验证。

- 设定了权限的有效时间，超过有效时间需要重新验证。

2.文件完整性验证:

- 使用 SHA256 哈希算法计算文件的哈希值，并与存储的哈希值进行比较，验证文件是否被修改或损坏。

## 使用方式:

1.运行程序: 直接运行 python 权限管理.py 即可。

2.查看操作列表: 程序会显示可用的操作和每个操作所需的权限级别。

3.输入命令:

- 输入 查看数据、修改配置 或 核心操作 等命令，执行相应的操作。

- 输入 退出 或 结束 退出程序。

4.输入密钥:

当需要权限才能执行操作时，程序会提示用户输入密钥。

输入正确的密钥才能验证通过，否则操作会被拒绝。


## 示例:

1.运行程序，程序会显示可用的操作列表和所需的权限级别。

2.输入正确的密钥，验证成功后，程序会提示正在修改配置。

3.输入错误的密钥，验证失败，程序会提示权限不足。

## 注意:

- 这段代码仅供参考，实际应用中需要使用更安全的密钥存储和验证方式，例如数据库或安全令牌。

- 可以根据实际情况修改权限级别、操作定义和密钥存储方式。

- 可以添加更多功能，例如记录操作日志、设置用户组等等。
- 

## my_package/Logging.py

🔎
```bash
my_package/Logging.py
```

这个代码提供了一个自定义的日志记录系统，它包含读取和查看日志文件的功能。

# 功能介绍：

- getLogger(name): 创建一个名为 name 的日志记录器，并将其写入名为 logging.txt 的文件。它会自动将日志文件切割成大小为 10MB 的文件，并在午夜进行切割。

- tail(filepath, n=10): 实现了 tail -n 命令的功能，可以高效地从文件读取最后 n 行，适合处理大型文件。

- readLog(lines=200): 从日志文件 logging.txt 中读取最近的 lines 行（默认 200 行）并返回内容。

## 使用方式:

1.记录日志：
```bash
import my_logging  # 假设你将代码保存在名为 `my_logging.py` 的文件中

logger = my_logging.getLogger(__name__)  # 获取当前模块的日志记录器

logger.info("这是一条信息性消息")
logger.warning("可能出现错误")
logger.error("发生了错误！") 
```
2.读取日志：
```hash
log_content = my_logging.readLog(lines=50)  # 读取最近的 50 行
print(log_content) 
```

## 简而言之:

- 使用 getLogger() 函数来创建日志记录器。

- 使用 logger.info(), logger.warning(), logger.error() 等方法来记录不同类型的日志消息。

- 使用 readLog() 函数来读取日志文件中的内容。

## 优点：

- 结构化的日志：自定义的日志记录器提供了统一的日志格式，包含时间戳、模块名称、文件名、行号和严重级别等信息。

- 文件轮转：日志文件会自动切割，防止文件过大。

- 尾部查看功能：tail() 函数允许您轻松查看最新的日志条目，无需滚动整个文件。

## 注意:

- 您可以更改 TEMP_PATH 变量来指定日志文件的存储位置。

- 您可以根据文件大小和系统资源调整 PAGE 大小（4096 字节）来优化读取性能。

- 您可以更改 getLogger() 函数中的日志级别来控制日志文件的详细程度（例如，logging.DEBUG 会记录更多信息）
- 
## my_package/crawler.py

🔎
```bash
my_package/crawler.py
```

这个程序是一个图片搜索和爬取工具，它可以帮助你从网页上搜索和下载图片。


## 功能介绍:

- 图片搜索: 根据关键词搜索Bing图片，列出搜索结果，并显示图片链接。

- 图片下载: 允许用户选择想要下载的图片，可以下载单个图片或多个图片，使用多线程下载图片，提高下载速度。

- 网站爬取: 根据输入的网址爬取网站上的所有图片，可以设置爬取深度，限制爬取范围。

- 错误处理: 包含错误处理机制，可以捕获并处理异常情况。

## 使用方法
1.安装依赖库：
```bash
  pip install requests beautifulsoup4 redis scrapy pillow
```

2.运行程序:
```bash
   python crawler.py 
```

3.输入搜索关键词或网址:

- 如果要搜索图片，输入想要搜索的关键词。

- 如果要爬取网站上的图片，输入网站的网址。
- 
4.下载图片:

- 程序会将下载的图片保存到 ./downloaded/ 文件夹中。
- 
## 示例

- 搜索“猫”图片：
```bash
python crawler.py
```
输入搜索关键词：猫
程序会显示搜索结果，用户可以选择要下载的图片编号。

- 爬取网站 https://www.example.com 上的图片：
```bash
python crawler.py
```

输入网址：https://www.example.com
程序会自动爬取网站上的所有图片。
